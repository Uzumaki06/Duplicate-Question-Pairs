{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96585e84-62fa-4179-9ca3-85602de2b1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e780ae0-960e-4adc-8683-eded2a1784a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dafa6683-6728-4981-a221-fa9e9a30d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df=df.sample(30000,random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "188a24ea-27e5-445f-b278-421c0184e5ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16eeeda0-07a5-405d-8e7a-58dcce62c504",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398782</th>\n",
       "      <td>398782</td>\n",
       "      <td>496695</td>\n",
       "      <td>532029</td>\n",
       "      <td>What is the best marketing automation tool for...</td>\n",
       "      <td>What is the best marketing automation tool for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>115086</td>\n",
       "      <td>187729</td>\n",
       "      <td>187730</td>\n",
       "      <td>I am poor but I want to invest. What should I do?</td>\n",
       "      <td>I am quite poor and I want to be very rich. Wh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327711</th>\n",
       "      <td>327711</td>\n",
       "      <td>454161</td>\n",
       "      <td>454162</td>\n",
       "      <td>I am from India and live abroad. I met a guy f...</td>\n",
       "      <td>T.I.E.T to Thapar University to Thapar Univers...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367788</th>\n",
       "      <td>367788</td>\n",
       "      <td>498109</td>\n",
       "      <td>491396</td>\n",
       "      <td>Why do so many people in the U.S. hate the sou...</td>\n",
       "      <td>My boyfriend doesnt feel guilty when he hurts ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151235</th>\n",
       "      <td>151235</td>\n",
       "      <td>237843</td>\n",
       "      <td>50930</td>\n",
       "      <td>Consequences of Bhopal gas tragedy?</td>\n",
       "      <td>What was the reason behind the Bhopal gas trag...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "398782  398782  496695  532029   \n",
       "115086  115086  187729  187730   \n",
       "327711  327711  454161  454162   \n",
       "367788  367788  498109  491396   \n",
       "151235  151235  237843   50930   \n",
       "\n",
       "                                                question1  \\\n",
       "398782  What is the best marketing automation tool for...   \n",
       "115086  I am poor but I want to invest. What should I do?   \n",
       "327711  I am from India and live abroad. I met a guy f...   \n",
       "367788  Why do so many people in the U.S. hate the sou...   \n",
       "151235                Consequences of Bhopal gas tragedy?   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "398782  What is the best marketing automation tool for...             1  \n",
       "115086  I am quite poor and I want to be very rich. Wh...             0  \n",
       "327711  T.I.E.T to Thapar University to Thapar Univers...             0  \n",
       "367788  My boyfriend doesnt feel guilty when he hurts ...             0  \n",
       "151235  What was the reason behind the Bhopal gas trag...             0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49171357-58bf-4569-937e-dfc10212e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e36e3d5-d891-4df2-a139-db126b536faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(ques):\n",
    "    ques = str(ques).lower().strip()\n",
    "\n",
    "    #Replacing some of the characters \n",
    "    ques=ques.replace('@', 'at')\n",
    "    ques=ques.replace('$', 'dollar')\n",
    "    ques=ques.replace('%', 'percent')\n",
    "    ques=ques.replace('&', 'and')\n",
    "    ques=ques.replace('=', 'equals')\n",
    "\n",
    "    ques=ques.replace('[math]','') # math occurs around 900 times in the whole dataset and means nothing\n",
    "\n",
    "    contractions = { \n",
    "    \n",
    "                \"ain't\": \"am not / are not / is not / has not / have not\",\n",
    "                \"aren't\": \"are not / am not\",\n",
    "                \"can't\": \"cannot\",\n",
    "                \"can't've\": \"cannot have\",\n",
    "                \"'cause\": \"because\",\n",
    "                \"could've\": \"could have\",\n",
    "                \"couldn't\": \"could not\",\n",
    "                \"couldn't've\": \"could not have\",\n",
    "                \"didn't\": \"did not\",\n",
    "                \"doesn't\": \"does not\",\n",
    "                \"don't\": \"do not\",\n",
    "                \"hadn't\": \"had not\",\n",
    "                \"hadn't've\": \"had not have\",\n",
    "                \"hasn't\": \"has not\",\n",
    "                \"haven't\": \"have not\",\n",
    "                \"he'd\": \"he had / he would\",\n",
    "                \"he'd've\": \"he would have\",\n",
    "                \"he'll\": \"he shall / he will\",\n",
    "                \"he'll've\": \"he shall have / he will have\",\n",
    "                \"he's\": \"he has / he is\",\n",
    "                \"how'd\": \"how did\",\n",
    "                \"how'd'y\": \"how do you\",\n",
    "                \"how'll\": \"how will\",\n",
    "                \"how's\": \"how has / how is / how does\",\n",
    "                \"I'd\": \"I had / I would\",\n",
    "                \"I'd've\": \"I would have\",\n",
    "                \"I'll\": \"I shall / I will\",\n",
    "                \"I'll've\": \"I shall have / I will have\",\n",
    "                \"I'm\": \"I am\",\n",
    "                \"I've\": \"I have\",\n",
    "                \"isn't\": \"is not\",\n",
    "                \"it'd\": \"it had / it would\",\n",
    "                \"it'd've\": \"it would have\",\n",
    "                \"it'll\": \"it shall / it will\",\n",
    "                \"it'll've\": \"it shall have / it will have\",\n",
    "                \"it's\": \"it has / it is\",\n",
    "                \"let's\": \"let us\",\n",
    "                \"ma'am\": \"madam\",\n",
    "                \"mayn't\": \"may not\",\n",
    "                \"might've\": \"might have\",\n",
    "                \"mightn't\": \"might not\",\n",
    "                \"mightn't've\": \"might not have\",\n",
    "                \"must've\": \"must have\",\n",
    "                \"mustn't\": \"must not\",\n",
    "                \"mustn't've\": \"must not have\",\n",
    "                \"needn't\": \"need not\",\n",
    "                \"needn't've\": \"need not have\",\n",
    "                \"o'clock\": \"of the clock\",\n",
    "                \"oughtn't\": \"ought not\",\n",
    "                \"oughtn't've\": \"ought not have\",\n",
    "                \"shan't\": \"shall not\",\n",
    "                \"sha'n't\": \"shall not\",\n",
    "                \"shan't've\": \"shall not have\",\n",
    "                \"she'd\": \"she had / she would\",\n",
    "                \"she'd've\": \"she would have\",\n",
    "                \"she'll\": \"she shall / she will\",\n",
    "                \"she'll've\": \"she shall have / she will have\",\n",
    "                \"she's\": \"she has / she is\",\n",
    "                \"should've\": \"should have\",\n",
    "                \"shouldn't\": \"should not\",\n",
    "                \"shouldn't've\": \"should not have\",\n",
    "                \"so've\": \"so have\",\n",
    "                \"so's\": \"so as / so is\",\n",
    "                \"that'd\": \"that would / that had\",\n",
    "                \"that'd've\": \"that would have\",\n",
    "                \"that's\": \"that has / that is\",\n",
    "                \"there'd\": \"there had / there would\",\n",
    "                \"there'd've\": \"there would have\",\n",
    "                \"there's\": \"there has / there is\",\n",
    "                \"they'd\": \"they had / they would\",\n",
    "                \"they'd've\": \"they would have\",\n",
    "                \"they'll\": \"they shall / they will\",\n",
    "                \"they'll've\": \"they shall have / they will have\",\n",
    "                \"they're\": \"they are\",\n",
    "                \"they've\": \"they have\",\n",
    "                \"to've\": \"to have\",\n",
    "                \"wasn't\": \"was not\",\n",
    "                \"we'd\": \"we had / we would\",\n",
    "                \"we'd've\": \"we would have\",\n",
    "                \"we'll\": \"we will\",\n",
    "                \"we'll've\": \"we will have\",\n",
    "                \"we're\": \"we are\",\n",
    "                \"we've\": \"we have\",\n",
    "                \"weren't\": \"were not\",\n",
    "                \"what'll\": \"what shall / what will\",\n",
    "                \"what'll've\": \"what shall have / what will have\",\n",
    "                \"what're\": \"what are\",\n",
    "                \"what's\": \"what has / what is\",\n",
    "                \"what've\": \"what have\",\n",
    "                \"when's\": \"when has / when is\",\n",
    "                \"when've\": \"when have\",\n",
    "                \"where'd\": \"where did\",\n",
    "                \"where's\": \"where has / where is\",\n",
    "                \"where've\": \"where have\",\n",
    "                \"who'll\": \"who shall / who will\",\n",
    "                \"who'll've\": \"who shall have / who will have\",\n",
    "                \"who's\": \"who has / who is\",\n",
    "                \"who've\": \"who have\",\n",
    "                \"why's\": \"why has / why is\",\n",
    "                \"why've\": \"why have\",\n",
    "                \"will've\": \"will have\",\n",
    "                \"won't\": \"will not\",\n",
    "                \"won't've\": \"will not have\",\n",
    "                \"would've\": \"would have\",\n",
    "                \"wouldn't\": \"would not\",\n",
    "                \"wouldn't've\": \"would not have\",\n",
    "                \"y'all\": \"you all\",\n",
    "                \"y'all'd\": \"you all would\",\n",
    "                \"y'all'd've\": \"you all would have\",\n",
    "                \"y'all're\": \"you all are\",\n",
    "                \"y'all've\": \"you all have\",\n",
    "                \"you'd\": \"you had / you would\",\n",
    "                \"you'd've\": \"you would have\",\n",
    "                \"you'll\": \"you shall / you will\",\n",
    "                \"you'll've\": \"you shall have / you will have\",\n",
    "                \"you're\": \"you are\",\n",
    "                \"you've\": \"you have\"\n",
    "                }\n",
    "    ques_deconstruct=[] \n",
    "    for word in ques.split():\n",
    "        if word in contractions:\n",
    "            word=contractions[word]\n",
    "            \n",
    "        ques_deconstruct.append(word)\n",
    "\n",
    "    ques = ' '.join(ques_deconstruct)\n",
    "    ques = ques.replace(\"ve\",\"have\")\n",
    "    ques = ques.replace(\"re\",\"are\")\n",
    "    ques = ques.replace(\"'ll\",\"will\")\n",
    "    ques = ques.replace(\"n't\",\"not\")\n",
    "\n",
    "    #Remove html tags using beautifulsoup\n",
    "    ques = BeautifulSoup(ques)\n",
    "    ques = ques.get_text()\n",
    "\n",
    "     #Removing all the punctuations from the dataset\n",
    "    punc = re.compile('\\W')\n",
    "    ques=re.sub(punc, ' ',ques).strip()\n",
    "\n",
    "  \n",
    "    return ques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7e32f24e-2f71-4543-a633-5656d3bc64a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['question1'] = new_df['question1'].apply(preprocess)\n",
    "new_df['question2'] = new_df['question2'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b97670d-e503-4c68-ae6f-525bd7cb5910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398782</th>\n",
       "      <td>398782</td>\n",
       "      <td>496695</td>\n",
       "      <td>532029</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>115086</td>\n",
       "      <td>187729</td>\n",
       "      <td>187730</td>\n",
       "      <td>i am poor but i want to inhavest  what should ...</td>\n",
       "      <td>i am quite poor and i want to be havery rich  ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327711</th>\n",
       "      <td>327711</td>\n",
       "      <td>454161</td>\n",
       "      <td>454162</td>\n",
       "      <td>i am from india and lihave abroad  i met a guy...</td>\n",
       "      <td>t i e t to thapar unihaversity to thapar uniha...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367788</th>\n",
       "      <td>367788</td>\n",
       "      <td>498109</td>\n",
       "      <td>491396</td>\n",
       "      <td>why do so many people in the u s  hate the sou...</td>\n",
       "      <td>my boyfriend doesnt feel guilty when he hurts ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151235</th>\n",
       "      <td>151235</td>\n",
       "      <td>237843</td>\n",
       "      <td>50930</td>\n",
       "      <td>consequences of bhopal gas tragedy</td>\n",
       "      <td>what was the areason behind the bhopal gas tra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "398782  398782  496695  532029   \n",
       "115086  115086  187729  187730   \n",
       "327711  327711  454161  454162   \n",
       "367788  367788  498109  491396   \n",
       "151235  151235  237843   50930   \n",
       "\n",
       "                                                question1  \\\n",
       "398782  what is the best marketing automation tool for...   \n",
       "115086  i am poor but i want to inhavest  what should ...   \n",
       "327711  i am from india and lihave abroad  i met a guy...   \n",
       "367788  why do so many people in the u s  hate the sou...   \n",
       "151235                 consequences of bhopal gas tragedy   \n",
       "\n",
       "                                                question2  is_duplicate  \n",
       "398782  what is the best marketing automation tool for...             1  \n",
       "115086  i am quite poor and i want to be havery rich  ...             0  \n",
       "327711  t i e t to thapar unihaversity to thapar uniha...             0  \n",
       "367788  my boyfriend doesnt feel guilty when he hurts ...             0  \n",
       "151235  what was the areason behind the bhopal gas tra...             0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40868d7b-478b-455d-974f-3e8f28b85bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['q1_len'] = new_df['question1'].str.len()\n",
    "new_df['q2_len'] = new_df['question2'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2935c8e1-4f21-461b-a18e-26f99eb2150f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['q1_words'] = new_df['question1'].apply(lambda row: len(row.split(\" \")))\n",
    "new_df['q2 _words'] = new_df['question2'].apply(lambda row: len(row.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "afce468e-6fbb-4465-9c64-6a440f9e1956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>q1_words</th>\n",
       "      <th>q2 _words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398782</th>\n",
       "      <td>398782</td>\n",
       "      <td>496695</td>\n",
       "      <td>532029</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>115086</td>\n",
       "      <td>187729</td>\n",
       "      <td>187730</td>\n",
       "      <td>i am poor but i want to inhavest  what should ...</td>\n",
       "      <td>i am quite poor and i want to be havery rich  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>58</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327711</th>\n",
       "      <td>327711</td>\n",
       "      <td>454161</td>\n",
       "      <td>454162</td>\n",
       "      <td>i am from india and lihave abroad  i met a guy...</td>\n",
       "      <td>t i e t to thapar unihaversity to thapar uniha...</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>123</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367788</th>\n",
       "      <td>367788</td>\n",
       "      <td>498109</td>\n",
       "      <td>491396</td>\n",
       "      <td>why do so many people in the u s  hate the sou...</td>\n",
       "      <td>my boyfriend doesnt feel guilty when he hurts ...</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>145</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151235</th>\n",
       "      <td>151235</td>\n",
       "      <td>237843</td>\n",
       "      <td>50930</td>\n",
       "      <td>consequences of bhopal gas tragedy</td>\n",
       "      <td>what was the areason behind the bhopal gas tra...</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "398782  398782  496695  532029   \n",
       "115086  115086  187729  187730   \n",
       "327711  327711  454161  454162   \n",
       "367788  367788  498109  491396   \n",
       "151235  151235  237843   50930   \n",
       "\n",
       "                                                question1  \\\n",
       "398782  what is the best marketing automation tool for...   \n",
       "115086  i am poor but i want to inhavest  what should ...   \n",
       "327711  i am from india and lihave abroad  i met a guy...   \n",
       "367788  why do so many people in the u s  hate the sou...   \n",
       "151235                 consequences of bhopal gas tragedy   \n",
       "\n",
       "                                                question2  is_duplicate  \\\n",
       "398782  what is the best marketing automation tool for...             1   \n",
       "115086  i am quite poor and i want to be havery rich  ...             0   \n",
       "327711  t i e t to thapar unihaversity to thapar uniha...             0   \n",
       "367788  my boyfriend doesnt feel guilty when he hurts ...             0   \n",
       "151235  what was the areason behind the bhopal gas tra...             0   \n",
       "\n",
       "        q1_len  q2_len  q1_words  q2 _words  \n",
       "398782      75      76        13         13  \n",
       "115086      50      58        13         16  \n",
       "327711     106     123        28         21  \n",
       "367788      58     145        14         32  \n",
       "151235      34      50         5          9  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b9efa4a0-f6b4-4586-9af1-00bb8f70a061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_Words(row):\n",
    "    w1 = set(map(lambda word : word.lower().strip(), row['question1'].split(\" \"))) # Finding common words in q1 and q2\n",
    "    w2 = set(map(lambda word : word.lower().strip(), row['question2'].split(\" \")))\n",
    "    return len(w1&w2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a32efc0-50a6-4758-be94-3f18dd13c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['common_words'] = new_df.apply(common_Words,axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a571aa93-be2f-457e-8cc1-14a36b056338",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>q1_words</th>\n",
       "      <th>q2 _words</th>\n",
       "      <th>common_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398782</th>\n",
       "      <td>398782</td>\n",
       "      <td>496695</td>\n",
       "      <td>532029</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>115086</td>\n",
       "      <td>187729</td>\n",
       "      <td>187730</td>\n",
       "      <td>i am poor but i want to inhavest  what should ...</td>\n",
       "      <td>i am quite poor and i want to be havery rich  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>58</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327711</th>\n",
       "      <td>327711</td>\n",
       "      <td>454161</td>\n",
       "      <td>454162</td>\n",
       "      <td>i am from india and lihave abroad  i met a guy...</td>\n",
       "      <td>t i e t to thapar unihaversity to thapar uniha...</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>123</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367788</th>\n",
       "      <td>367788</td>\n",
       "      <td>498109</td>\n",
       "      <td>491396</td>\n",
       "      <td>why do so many people in the u s  hate the sou...</td>\n",
       "      <td>my boyfriend doesnt feel guilty when he hurts ...</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>145</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151235</th>\n",
       "      <td>151235</td>\n",
       "      <td>237843</td>\n",
       "      <td>50930</td>\n",
       "      <td>consequences of bhopal gas tragedy</td>\n",
       "      <td>what was the areason behind the bhopal gas tra...</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "398782  398782  496695  532029   \n",
       "115086  115086  187729  187730   \n",
       "327711  327711  454161  454162   \n",
       "367788  367788  498109  491396   \n",
       "151235  151235  237843   50930   \n",
       "\n",
       "                                                question1  \\\n",
       "398782  what is the best marketing automation tool for...   \n",
       "115086  i am poor but i want to inhavest  what should ...   \n",
       "327711  i am from india and lihave abroad  i met a guy...   \n",
       "367788  why do so many people in the u s  hate the sou...   \n",
       "151235                 consequences of bhopal gas tragedy   \n",
       "\n",
       "                                                question2  is_duplicate  \\\n",
       "398782  what is the best marketing automation tool for...             1   \n",
       "115086  i am quite poor and i want to be havery rich  ...             0   \n",
       "327711  t i e t to thapar unihaversity to thapar uniha...             0   \n",
       "367788  my boyfriend doesnt feel guilty when he hurts ...             0   \n",
       "151235  what was the areason behind the bhopal gas tra...             0   \n",
       "\n",
       "        q1_len  q2_len  q1_words  q2 _words  common_words  \n",
       "398782      75      76        13         13            12  \n",
       "115086      50      58        13         16             8  \n",
       "327711     106     123        28         21             4  \n",
       "367788      58     145        14         32             1  \n",
       "151235      34      50         5          9             3  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "247560a0-c83e-4573-bc29-5fee9a9d699c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_words(row):\n",
    "    w1 = set(map(lambda word : word.lower().strip(), row['question1'].split(\" \")))\n",
    "    w2 = set(map(lambda word : word.lower().strip(), row['question2'].split(\" \")))\n",
    "    return (len(w1)+len(w2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2d04f7ce-d3b6-4e99-9ea4-85087bbe51d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>q1_words</th>\n",
       "      <th>q2 _words</th>\n",
       "      <th>common_words</th>\n",
       "      <th>ttl_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398782</th>\n",
       "      <td>398782</td>\n",
       "      <td>496695</td>\n",
       "      <td>532029</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>115086</td>\n",
       "      <td>187729</td>\n",
       "      <td>187730</td>\n",
       "      <td>i am poor but i want to inhavest  what should ...</td>\n",
       "      <td>i am quite poor and i want to be havery rich  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>58</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327711</th>\n",
       "      <td>327711</td>\n",
       "      <td>454161</td>\n",
       "      <td>454162</td>\n",
       "      <td>i am from india and lihave abroad  i met a guy...</td>\n",
       "      <td>t i e t to thapar unihaversity to thapar uniha...</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>123</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367788</th>\n",
       "      <td>367788</td>\n",
       "      <td>498109</td>\n",
       "      <td>491396</td>\n",
       "      <td>why do so many people in the u s  hate the sou...</td>\n",
       "      <td>my boyfriend doesnt feel guilty when he hurts ...</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>145</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151235</th>\n",
       "      <td>151235</td>\n",
       "      <td>237843</td>\n",
       "      <td>50930</td>\n",
       "      <td>consequences of bhopal gas tragedy</td>\n",
       "      <td>what was the areason behind the bhopal gas tra...</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "398782  398782  496695  532029   \n",
       "115086  115086  187729  187730   \n",
       "327711  327711  454161  454162   \n",
       "367788  367788  498109  491396   \n",
       "151235  151235  237843   50930   \n",
       "\n",
       "                                                question1  \\\n",
       "398782  what is the best marketing automation tool for...   \n",
       "115086  i am poor but i want to inhavest  what should ...   \n",
       "327711  i am from india and lihave abroad  i met a guy...   \n",
       "367788  why do so many people in the u s  hate the sou...   \n",
       "151235                 consequences of bhopal gas tragedy   \n",
       "\n",
       "                                                question2  is_duplicate  \\\n",
       "398782  what is the best marketing automation tool for...             1   \n",
       "115086  i am quite poor and i want to be havery rich  ...             0   \n",
       "327711  t i e t to thapar unihaversity to thapar uniha...             0   \n",
       "367788  my boyfriend doesnt feel guilty when he hurts ...             0   \n",
       "151235  what was the areason behind the bhopal gas tra...             0   \n",
       "\n",
       "        q1_len  q2_len  q1_words  q2 _words  common_words  ttl_words  \n",
       "398782      75      76        13         13            12         26  \n",
       "115086      50      58        13         16             8         24  \n",
       "327711     106     123        28         21             4         38  \n",
       "367788      58     145        14         32             1         34  \n",
       "151235      34      50         5          9             3         13  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df['ttl_words'] = new_df.apply(total_words,axis=1)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd977e1c-26f6-4218-9a4b-98f59aeed0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['word_share'] = round(new_df['common_words'] / new_df['ttl_words'],3) # Here 3 means round off the value upto 3 decimal places"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79cce8fb-3ac5-4b82-8bf0-6f6735875816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>q1_words</th>\n",
       "      <th>q2 _words</th>\n",
       "      <th>common_words</th>\n",
       "      <th>ttl_words</th>\n",
       "      <th>word_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398782</th>\n",
       "      <td>398782</td>\n",
       "      <td>496695</td>\n",
       "      <td>532029</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>0.462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>115086</td>\n",
       "      <td>187729</td>\n",
       "      <td>187730</td>\n",
       "      <td>i am poor but i want to inhavest  what should ...</td>\n",
       "      <td>i am quite poor and i want to be havery rich  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>58</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327711</th>\n",
       "      <td>327711</td>\n",
       "      <td>454161</td>\n",
       "      <td>454162</td>\n",
       "      <td>i am from india and lihave abroad  i met a guy...</td>\n",
       "      <td>t i e t to thapar unihaversity to thapar uniha...</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>123</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>0.105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367788</th>\n",
       "      <td>367788</td>\n",
       "      <td>498109</td>\n",
       "      <td>491396</td>\n",
       "      <td>why do so many people in the u s  hate the sou...</td>\n",
       "      <td>my boyfriend doesnt feel guilty when he hurts ...</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>145</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151235</th>\n",
       "      <td>151235</td>\n",
       "      <td>237843</td>\n",
       "      <td>50930</td>\n",
       "      <td>consequences of bhopal gas tragedy</td>\n",
       "      <td>what was the areason behind the bhopal gas tra...</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0.231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "398782  398782  496695  532029   \n",
       "115086  115086  187729  187730   \n",
       "327711  327711  454161  454162   \n",
       "367788  367788  498109  491396   \n",
       "151235  151235  237843   50930   \n",
       "\n",
       "                                                question1  \\\n",
       "398782  what is the best marketing automation tool for...   \n",
       "115086  i am poor but i want to inhavest  what should ...   \n",
       "327711  i am from india and lihave abroad  i met a guy...   \n",
       "367788  why do so many people in the u s  hate the sou...   \n",
       "151235                 consequences of bhopal gas tragedy   \n",
       "\n",
       "                                                question2  is_duplicate  \\\n",
       "398782  what is the best marketing automation tool for...             1   \n",
       "115086  i am quite poor and i want to be havery rich  ...             0   \n",
       "327711  t i e t to thapar unihaversity to thapar uniha...             0   \n",
       "367788  my boyfriend doesnt feel guilty when he hurts ...             0   \n",
       "151235  what was the areason behind the bhopal gas tra...             0   \n",
       "\n",
       "        q1_len  q2_len  q1_words  q2 _words  common_words  ttl_words  \\\n",
       "398782      75      76        13         13            12         26   \n",
       "115086      50      58        13         16             8         24   \n",
       "327711     106     123        28         21             4         38   \n",
       "367788      58     145        14         32             1         34   \n",
       "151235      34      50         5          9             3         13   \n",
       "\n",
       "        word_share  \n",
       "398782       0.462  \n",
       "115086       0.333  \n",
       "327711       0.105  \n",
       "367788       0.029  \n",
       "151235       0.231  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a14188e4-3a5a-425a-a047-82ad0ec33ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Features\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def fetch_token_features(row):\n",
    "    \n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    SAFE_DIV = 0.0001 \n",
    "\n",
    "    STOP_WORDS = stopwords.words(\"english\")\n",
    "    \n",
    "    token_features = [0.0]*8\n",
    "    \n",
    "    # Converting the Sentence into Tokens: \n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "    \n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return token_features\n",
    "\n",
    "    # Get the non-stopwords in Questions\n",
    "    q1_words = set([word for word in q1_tokens if word not in STOP_WORDS])\n",
    "    q2_words = set([word for word in q2_tokens if word not in STOP_WORDS])\n",
    "    \n",
    "    #Get the stopwords in Questions\n",
    "    q1_stops = set([word for word in q1_tokens if word in STOP_WORDS])\n",
    "    q2_stops = set([word for word in q2_tokens if word in STOP_WORDS])\n",
    "    \n",
    "    # Get the common non-stopwords from Question pair\n",
    "    common_word_count = len(q1_words.intersection(q2_words))\n",
    "    \n",
    "    # Get the common stopwords from Question pair\n",
    "    common_stop_count = len(q1_stops.intersection(q2_stops))\n",
    "    \n",
    "    # Get the common Tokens from Question pair\n",
    "    common_token_count = len(set(q1_tokens).intersection(set(q2_tokens)))\n",
    "    \n",
    "    \n",
    "    token_features[0] = common_word_count / (min(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[1] = common_word_count / (max(len(q1_words), len(q2_words)) + SAFE_DIV)\n",
    "    token_features[2] = common_stop_count / (min(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[3] = common_stop_count / (max(len(q1_stops), len(q2_stops)) + SAFE_DIV)\n",
    "    token_features[4] = common_token_count / (min(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    token_features[5] = common_token_count / (max(len(q1_tokens), len(q2_tokens)) + SAFE_DIV)\n",
    "    \n",
    "    # Last word of both question is same or not\n",
    "    token_features[6] = int(q1_tokens[-1] == q2_tokens[-1])\n",
    "    \n",
    "    # First word of both question is same or not\n",
    "    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n",
    "    \n",
    "    return token_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "049c5314-3d86-408c-8039-baaab96d2e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_features = new_df.apply(fetch_token_features, axis=1)\n",
    "\n",
    "new_df[\"cwc_min\"]       = list(map(lambda x: x[0], token_features))\n",
    "new_df[\"cwc_max\"]       = list(map(lambda x: x[1], token_features))\n",
    "new_df[\"csc_min\"]       = list(map(lambda x: x[2], token_features))\n",
    "new_df[\"csc_max\"]       = list(map(lambda x: x[3], token_features))\n",
    "new_df[\"ctc_min\"]       = list(map(lambda x: x[4], token_features))\n",
    "new_df[\"ctc_max\"]       = list(map(lambda x: x[5], token_features))\n",
    "new_df[\"last_word_eq\"]  = list(map(lambda x: x[6], token_features))\n",
    "new_df[\"first_word_eq\"] = list(map(lambda x: x[7], token_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2ce41e21-b002-41c6-8e08-4b29ba350d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>q1_words</th>\n",
       "      <th>q2 _words</th>\n",
       "      <th>...</th>\n",
       "      <th>ttl_words</th>\n",
       "      <th>word_share</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>csc_min</th>\n",
       "      <th>csc_max</th>\n",
       "      <th>ctc_min</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398782</th>\n",
       "      <td>398782</td>\n",
       "      <td>496695</td>\n",
       "      <td>532029</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>26</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.874989</td>\n",
       "      <td>0.874989</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.999980</td>\n",
       "      <td>0.923070</td>\n",
       "      <td>0.923070</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>115086</td>\n",
       "      <td>187729</td>\n",
       "      <td>187730</td>\n",
       "      <td>i am poor but i want to inhavest  what should ...</td>\n",
       "      <td>i am quite poor and i want to be havery rich  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>58</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>0.714276</td>\n",
       "      <td>0.714276</td>\n",
       "      <td>0.583328</td>\n",
       "      <td>0.466664</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327711</th>\n",
       "      <td>327711</td>\n",
       "      <td>454161</td>\n",
       "      <td>454162</td>\n",
       "      <td>i am from india and lihave abroad  i met a guy...</td>\n",
       "      <td>t i e t to thapar unihaversity to thapar uniha...</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>123</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>38</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.428565</td>\n",
       "      <td>0.272725</td>\n",
       "      <td>0.149999</td>\n",
       "      <td>0.115384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367788</th>\n",
       "      <td>367788</td>\n",
       "      <td>498109</td>\n",
       "      <td>491396</td>\n",
       "      <td>why do so many people in the u s  hate the sou...</td>\n",
       "      <td>my boyfriend doesnt feel guilty when he hurts ...</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>145</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151235</th>\n",
       "      <td>151235</td>\n",
       "      <td>237843</td>\n",
       "      <td>50930</td>\n",
       "      <td>consequences of bhopal gas tragedy</td>\n",
       "      <td>what was the areason behind the bhopal gas tra...</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>13</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>0.333330</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "398782  398782  496695  532029   \n",
       "115086  115086  187729  187730   \n",
       "327711  327711  454161  454162   \n",
       "367788  367788  498109  491396   \n",
       "151235  151235  237843   50930   \n",
       "\n",
       "                                                question1  \\\n",
       "398782  what is the best marketing automation tool for...   \n",
       "115086  i am poor but i want to inhavest  what should ...   \n",
       "327711  i am from india and lihave abroad  i met a guy...   \n",
       "367788  why do so many people in the u s  hate the sou...   \n",
       "151235                 consequences of bhopal gas tragedy   \n",
       "\n",
       "                                                question2  is_duplicate  \\\n",
       "398782  what is the best marketing automation tool for...             1   \n",
       "115086  i am quite poor and i want to be havery rich  ...             0   \n",
       "327711  t i e t to thapar unihaversity to thapar uniha...             0   \n",
       "367788  my boyfriend doesnt feel guilty when he hurts ...             0   \n",
       "151235  what was the areason behind the bhopal gas tra...             0   \n",
       "\n",
       "        q1_len  q2_len  q1_words  q2 _words  ...  ttl_words  word_share  \\\n",
       "398782      75      76        13         13  ...         26       0.462   \n",
       "115086      50      58        13         16  ...         24       0.333   \n",
       "327711     106     123        28         21  ...         38       0.105   \n",
       "367788      58     145        14         32  ...         34       0.029   \n",
       "151235      34      50         5          9  ...         13       0.231   \n",
       "\n",
       "         cwc_min   cwc_max   csc_min   csc_max   ctc_min   ctc_max  \\\n",
       "398782  0.874989  0.874989  0.999980  0.999980  0.923070  0.923070   \n",
       "115086  0.666644  0.399992  0.714276  0.714276  0.583328  0.466664   \n",
       "327711  0.000000  0.000000  0.428565  0.272725  0.149999  0.115384   \n",
       "367788  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "151235  0.749981  0.599988  0.000000  0.000000  0.599988  0.333330   \n",
       "\n",
       "        last_word_eq  first_word_eq  \n",
       "398782           1.0            1.0  \n",
       "115086           1.0            1.0  \n",
       "327711           0.0            0.0  \n",
       "367788           0.0            0.0  \n",
       "151235           1.0            0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac799d12-5da7-49ef-be61-763e53af7d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import distance\n",
    "\n",
    "def fetch_length_features(row):\n",
    "    \n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    length_features = [0.0]*3\n",
    "    \n",
    "    # Converting the Sentence into Tokens: \n",
    "    q1_tokens = q1.split()\n",
    "    q2_tokens = q2.split()\n",
    "    \n",
    "    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n",
    "        return length_features\n",
    "    \n",
    "    # Absolute length features\n",
    "    length_features[0] = abs(len(q1_tokens) - len(q2_tokens))\n",
    "    \n",
    "    #Average Token Length of both Questions\n",
    "    length_features[1] = (len(q1_tokens) + len(q2_tokens))/2\n",
    "    \n",
    "    strs = list(distance.lcsubstrings(q1, q2))\n",
    "    if strs:\n",
    "        length_features[2] = len(strs[0]) / (min(len(q1), len(q2)) + 1)\n",
    "    else:\n",
    "        length_features[2] = 0.0\n",
    "    \n",
    "    return length_features\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f92d8a15-cd75-4710-a0a5-d1592474379a",
   "metadata": {},
   "outputs": [],
   "source": [
    "length_features = new_df.apply(fetch_length_features, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "50894f25-d343-44a5-863e-5f739b82c45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df['abs_len_diff'] = list(map(lambda x: x[0], length_features))\n",
    "new_df['mean_len'] = list(map(lambda x: x[1], length_features))\n",
    "new_df['longest_substr_ratio'] = list(map(lambda x: x[2], length_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7fce0441-6552-4304-a6b6-0a16a8857ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def fetch_fuzzy_features(row):\n",
    "    \n",
    "    q1 = row['question1']\n",
    "    q2 = row['question2']\n",
    "    \n",
    "    fuzzy_features = [0.0]*4\n",
    "    \n",
    "    # fuzz_ratio\n",
    "    fuzzy_features[0] = fuzz.QRatio(q1, q2)\n",
    "\n",
    "    # fuzz_partial_ratio\n",
    "    fuzzy_features[1] = fuzz.partial_ratio(q1, q2)\n",
    "\n",
    "    # token_sort_ratio\n",
    "    fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)\n",
    "\n",
    "    # token_set_ratio\n",
    "    fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)\n",
    "\n",
    "    return fuzzy_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6cbe86e8-c443-44bd-a85c-0843b9d7f7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzy_features = new_df.apply(fetch_fuzzy_features, axis=1)\n",
    "\n",
    "# Creating new feature columns for fuzzy features\n",
    "new_df['fuzz_ratio'] = list(map(lambda x: x[0], fuzzy_features))\n",
    "new_df['fuzz_partial_ratio'] = list(map(lambda x: x[1], fuzzy_features))\n",
    "new_df['token_sort_ratio'] = list(map(lambda x: x[2], fuzzy_features))\n",
    "new_df['token_set_ratio'] = list(map(lambda x: x[3], fuzzy_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "235b5915-e08a-4346-b412-d622ae0947f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 28)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>q1_words</th>\n",
       "      <th>q2 _words</th>\n",
       "      <th>...</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>mean_len</th>\n",
       "      <th>longest_substr_ratio</th>\n",
       "      <th>fuzz_ratio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>token_sort_ratio</th>\n",
       "      <th>token_set_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398782</th>\n",
       "      <td>398782</td>\n",
       "      <td>496695</td>\n",
       "      <td>532029</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923070</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.855263</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>115086</td>\n",
       "      <td>187729</td>\n",
       "      <td>187730</td>\n",
       "      <td>i am poor but i want to inhavest  what should ...</td>\n",
       "      <td>i am quite poor and i want to be havery rich  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>58</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466664</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>70</td>\n",
       "      <td>68</td>\n",
       "      <td>62</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327711</th>\n",
       "      <td>327711</td>\n",
       "      <td>454161</td>\n",
       "      <td>454162</td>\n",
       "      <td>i am from india and lihave abroad  i met a guy...</td>\n",
       "      <td>t i e t to thapar unihaversity to thapar uniha...</td>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>123</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367788</th>\n",
       "      <td>367788</td>\n",
       "      <td>498109</td>\n",
       "      <td>491396</td>\n",
       "      <td>why do so many people in the u s  hate the sou...</td>\n",
       "      <td>my boyfriend doesnt feel guilty when he hurts ...</td>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>145</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>29</td>\n",
       "      <td>41</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151235</th>\n",
       "      <td>151235</td>\n",
       "      <td>237843</td>\n",
       "      <td>50930</td>\n",
       "      <td>consequences of bhopal gas tragedy</td>\n",
       "      <td>what was the areason behind the bhopal gas tra...</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333330</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>55</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id    qid1    qid2  \\\n",
       "398782  398782  496695  532029   \n",
       "115086  115086  187729  187730   \n",
       "327711  327711  454161  454162   \n",
       "367788  367788  498109  491396   \n",
       "151235  151235  237843   50930   \n",
       "\n",
       "                                                question1  \\\n",
       "398782  what is the best marketing automation tool for...   \n",
       "115086  i am poor but i want to inhavest  what should ...   \n",
       "327711  i am from india and lihave abroad  i met a guy...   \n",
       "367788  why do so many people in the u s  hate the sou...   \n",
       "151235                 consequences of bhopal gas tragedy   \n",
       "\n",
       "                                                question2  is_duplicate  \\\n",
       "398782  what is the best marketing automation tool for...             1   \n",
       "115086  i am quite poor and i want to be havery rich  ...             0   \n",
       "327711  t i e t to thapar unihaversity to thapar uniha...             0   \n",
       "367788  my boyfriend doesnt feel guilty when he hurts ...             0   \n",
       "151235  what was the areason behind the bhopal gas tra...             0   \n",
       "\n",
       "        q1_len  q2_len  q1_words  q2 _words  ...   ctc_max  last_word_eq  \\\n",
       "398782      75      76        13         13  ...  0.923070           1.0   \n",
       "115086      50      58        13         16  ...  0.466664           1.0   \n",
       "327711     106     123        28         21  ...  0.115384           0.0   \n",
       "367788      58     145        14         32  ...  0.000000           0.0   \n",
       "151235      34      50         5          9  ...  0.333330           1.0   \n",
       "\n",
       "        first_word_eq  abs_len_diff  mean_len  longest_substr_ratio  \\\n",
       "398782            1.0           0.0      13.0              0.855263   \n",
       "115086            1.0           3.0      13.5              0.215686   \n",
       "327711            0.0           6.0      23.0              0.046729   \n",
       "367788            0.0          17.0      21.5              0.050847   \n",
       "151235            0.0           4.0       7.0              0.542857   \n",
       "\n",
       "        fuzz_ratio  fuzz_partial_ratio  token_sort_ratio  token_set_ratio  \n",
       "398782          99                  99                99               99  \n",
       "115086          70                  68                62               74  \n",
       "327711          26                  30                32               38  \n",
       "367788          29                  41                23               30  \n",
       "151235          55                  70                45               69  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(new_df.shape)\n",
    "new_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2ca408ec-c1cc-4abb-9553-dd3e11e64837",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X = MinMaxScaler().fit_transform(new_df[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_len_diff' , 'mean_len' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])\n",
    "y = new_df['is_duplicate'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "429838f9-b5bf-45ac-990a-7ca400bfeb7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 30000 samples in 0.135s...\n",
      "[t-SNE] Computed neighbors for 30000 samples in 13.944s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 16000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 17000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 18000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 19000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 20000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 21000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 22000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 23000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 24000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 25000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 26000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 27000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 28000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 29000 / 30000\n",
      "[t-SNE] Computed conditional probabilities for sample 30000 / 30000\n",
      "[t-SNE] Mean sigma: 0.081161\n",
      "[t-SNE] Computed conditional probabilities in 2.252s\n",
      "[t-SNE] Iteration 50: error = 110.6415482, gradient norm = 0.0142405 (50 iterations in 70.539s)\n",
      "[t-SNE] Iteration 100: error = 86.6419754, gradient norm = 0.0044117 (50 iterations in 50.518s)\n",
      "[t-SNE] Iteration 150: error = 83.4727020, gradient norm = 0.0022234 (50 iterations in 42.687s)\n",
      "[t-SNE] Iteration 200: error = 82.2396851, gradient norm = 0.0014385 (50 iterations in 41.562s)\n",
      "[t-SNE] Iteration 250: error = 81.5597229, gradient norm = 0.0010697 (50 iterations in 42.673s)\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 81.559723\n",
      "[t-SNE] Iteration 300: error = 2.9863069, gradient norm = 0.0038122 (50 iterations in 51.654s)\n",
      "[t-SNE] Iteration 350: error = 2.3766408, gradient norm = 0.0030560 (50 iterations in 58.630s)\n",
      "[t-SNE] Iteration 400: error = 2.0680785, gradient norm = 0.0024317 (50 iterations in 58.834s)\n",
      "[t-SNE] Iteration 450: error = 1.8865926, gradient norm = 0.0020302 (50 iterations in 58.780s)\n",
      "[t-SNE] Iteration 500: error = 1.7663541, gradient norm = 0.0017420 (50 iterations in 55.219s)\n",
      "[t-SNE] Iteration 550: error = 1.6818906, gradient norm = 0.0015080 (50 iterations in 49.996s)\n",
      "[t-SNE] Iteration 600: error = 1.6199948, gradient norm = 0.0013197 (50 iterations in 51.062s)\n",
      "[t-SNE] Iteration 650: error = 1.5746272, gradient norm = 0.0011113 (50 iterations in 51.080s)\n",
      "[t-SNE] Iteration 700: error = 1.5424336, gradient norm = 0.0009049 (50 iterations in 51.341s)\n",
      "[t-SNE] Iteration 750: error = 1.5193458, gradient norm = 0.0007346 (50 iterations in 48.532s)\n",
      "[t-SNE] Iteration 800: error = 1.5026603, gradient norm = 0.0005893 (50 iterations in 50.231s)\n",
      "[t-SNE] Iteration 850: error = 1.4902155, gradient norm = 0.0004985 (50 iterations in 48.970s)\n",
      "[t-SNE] Iteration 900: error = 1.4799068, gradient norm = 0.0004351 (50 iterations in 54.610s)\n",
      "[t-SNE] Iteration 950: error = 1.4712024, gradient norm = 0.0003874 (50 iterations in 49.795s)\n",
      "[t-SNE] Iteration 1000: error = 1.4638083, gradient norm = 0.0003432 (50 iterations in 50.498s)\n",
      "[t-SNE] KL divergence after 1000 iterations: 1.463808\n"
     ]
    }
   ],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne3d = TSNE(\n",
    "    n_components=3,\n",
    "    init='random', # pca\n",
    "    random_state=101,\n",
    "    method='barnes_hut',\n",
    "    n_iter=1000,\n",
    "    verbose=2,\n",
    "    angle=0.5\n",
    ").fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c8e51214-2085-4bbe-b157-de1c4b8c7d00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398782</th>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "      <td>what is the best marketing automation tool for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>i am poor but i want to inhavest  what should ...</td>\n",
       "      <td>i am quite poor and i want to be havery rich  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327711</th>\n",
       "      <td>i am from india and lihave abroad  i met a guy...</td>\n",
       "      <td>t i e t to thapar unihaversity to thapar uniha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367788</th>\n",
       "      <td>why do so many people in the u s  hate the sou...</td>\n",
       "      <td>my boyfriend doesnt feel guilty when he hurts ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151235</th>\n",
       "      <td>consequences of bhopal gas tragedy</td>\n",
       "      <td>what was the areason behind the bhopal gas tra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                question1  \\\n",
       "398782  what is the best marketing automation tool for...   \n",
       "115086  i am poor but i want to inhavest  what should ...   \n",
       "327711  i am from india and lihave abroad  i met a guy...   \n",
       "367788  why do so many people in the u s  hate the sou...   \n",
       "151235                 consequences of bhopal gas tragedy   \n",
       "\n",
       "                                                question2  \n",
       "398782  what is the best marketing automation tool for...  \n",
       "115086  i am quite poor and i want to be havery rich  ...  \n",
       "327711  t i e t to thapar unihaversity to thapar uniha...  \n",
       "367788  my boyfriend doesnt feel guilty when he hurts ...  \n",
       "151235  what was the areason behind the bhopal gas tra...  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ques_df = new_df[['question1','question2']]\n",
    "ques_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7ef97342-250a-48b5-aa72-c62b68dcb992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 23)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>q1_words</th>\n",
       "      <th>q2 _words</th>\n",
       "      <th>common_words</th>\n",
       "      <th>ttl_words</th>\n",
       "      <th>word_share</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>...</th>\n",
       "      <th>ctc_max</th>\n",
       "      <th>last_word_eq</th>\n",
       "      <th>first_word_eq</th>\n",
       "      <th>abs_len_diff</th>\n",
       "      <th>mean_len</th>\n",
       "      <th>longest_substr_ratio</th>\n",
       "      <th>fuzz_ratio</th>\n",
       "      <th>fuzz_partial_ratio</th>\n",
       "      <th>token_sort_ratio</th>\n",
       "      <th>token_set_ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398782</th>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.874989</td>\n",
       "      <td>0.874989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.923070</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.855263</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "      <td>99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>58</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.466664</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>13.5</td>\n",
       "      <td>0.215686</td>\n",
       "      <td>70</td>\n",
       "      <td>68</td>\n",
       "      <td>62</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327711</th>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>123</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.115384</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0.046729</td>\n",
       "      <td>26</td>\n",
       "      <td>30</td>\n",
       "      <td>32</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367788</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>145</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>21.5</td>\n",
       "      <td>0.050847</td>\n",
       "      <td>29</td>\n",
       "      <td>41</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151235</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333330</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>55</td>\n",
       "      <td>70</td>\n",
       "      <td>45</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_duplicate  q1_len  q2_len  q1_words  q2 _words  common_words  \\\n",
       "398782             1      75      76        13         13            12   \n",
       "115086             0      50      58        13         16             8   \n",
       "327711             0     106     123        28         21             4   \n",
       "367788             0      58     145        14         32             1   \n",
       "151235             0      34      50         5          9             3   \n",
       "\n",
       "        ttl_words  word_share   cwc_min   cwc_max  ...   ctc_max  \\\n",
       "398782         26       0.462  0.874989  0.874989  ...  0.923070   \n",
       "115086         24       0.333  0.666644  0.399992  ...  0.466664   \n",
       "327711         38       0.105  0.000000  0.000000  ...  0.115384   \n",
       "367788         34       0.029  0.000000  0.000000  ...  0.000000   \n",
       "151235         13       0.231  0.749981  0.599988  ...  0.333330   \n",
       "\n",
       "        last_word_eq  first_word_eq  abs_len_diff  mean_len  \\\n",
       "398782           1.0            1.0           0.0      13.0   \n",
       "115086           1.0            1.0           3.0      13.5   \n",
       "327711           0.0            0.0           6.0      23.0   \n",
       "367788           0.0            0.0          17.0      21.5   \n",
       "151235           1.0            0.0           4.0       7.0   \n",
       "\n",
       "        longest_substr_ratio  fuzz_ratio  fuzz_partial_ratio  \\\n",
       "398782              0.855263          99                  99   \n",
       "115086              0.215686          70                  68   \n",
       "327711              0.046729          26                  30   \n",
       "367788              0.050847          29                  41   \n",
       "151235              0.542857          55                  70   \n",
       "\n",
       "        token_sort_ratio  token_set_ratio  \n",
       "398782                99               99  \n",
       "115086                62               74  \n",
       "327711                32               38  \n",
       "367788                23               30  \n",
       "151235                45               69  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = new_df.drop(columns=['id','qid1','qid2','question1','question2'])\n",
    "print(final_df.shape)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6a312904-b8cb-45ae-923d-98f57ff61b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in c:\\users\\arora\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.3.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\arora\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\arora\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\arora\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\users\\arora\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6494babe-aee7-4827-aef4-c61d2d624dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import triu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f74de9-1c12-4715-80b3-5c229ecdb8cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf29019e-27be-45b5-a601-2b530b3eda4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.Word2Vec(\n",
    "    window=10,\n",
    "    min_count=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "36a680b3-ea4b-46b6-aa30-0e0c22b2ab9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\arora\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\arora\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 300) (30000, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Ensure you have the stopwords and punkt datasets from nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Function to preprocess and tokenize text\n",
    "def preprocess(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Assuming ques_df is your DataFrame containing 'question1' and 'question2' columns\n",
    "# Example:\n",
    "# ques_df = pd.DataFrame({\n",
    "#     'question1': ['What is your name?', 'How are you?'],\n",
    "#     'question2': ['Where do you live?', 'What is your profession?']\n",
    "# })\n",
    "\n",
    "questions = list(ques_df['question1']) + list(ques_df['question2'])\n",
    "\n",
    "# Preprocess and tokenize the questions\n",
    "tokenized_questions = [preprocess(question) for question in questions]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=tokenized_questions, vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Convert tokenized questions back to strings for TF-IDF calculation\n",
    "questions = [' '.join(tokens) for tokens in tokenized_questions]\n",
    "\n",
    "# Calculate TF-IDF scores\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(questions)\n",
    "tfidf_feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Create a dictionary mapping words to their TF-IDF scores\n",
    "tfidf_scores = dict(zip(tfidf_feature_names, vectorizer.idf_))\n",
    "\n",
    "# Function to convert a question to a vector by averaging weighted word embeddings\n",
    "def question_to_weighted_vec(question, model, tfidf_scores):\n",
    "    tokens = preprocess(question)\n",
    "    vectors = []\n",
    "    weights = []\n",
    "    for word in tokens:\n",
    "        if word in model.wv and word in tfidf_scores:\n",
    "            vectors.append(model.wv[word] * tfidf_scores[word])\n",
    "            weights.append(tfidf_scores[word])\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.average(vectors, axis=0, weights=weights)\n",
    "\n",
    "# Convert each question to a vector\n",
    "q1_vecs = np.array([question_to_weighted_vec(q, w2v_model, tfidf_scores) for q in ques_df['question1']])\n",
    "q2_vecs = np.array([question_to_weighted_vec(q, w2v_model, tfidf_scores) for q in ques_df['question2']])\n",
    "\n",
    "# Now you have q1_vecs and q2_vecs as arrays of vectors\n",
    "print(q1_vecs.shape, q2_vecs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4c1af48-768f-4c6d-abc7-87035049110f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"from nltk.tokenize import word_tokenize\\nfrom nltk.corpus import stopwords\\nimport string\\nfrom gensim.models import Word2Vec\\n\\n# Ensure you have the stopwords and punkt datasets from nltk\\nimport nltk\\n\\n\\n# Function to preprocess and tokenize text\\ndef preprocess(text):\\n    stop_words = set(stopwords.words('english'))\\n    tokens = word_tokenize(text.lower())\\n    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\\n    return tokens\\n\\n# Assuming ques_df is your DataFrame containing 'question1' and 'question2' columns\\nquestions = list(ques_df['question1']) + list(ques_df['question2'])\\n\\n# Preprocess and tokenize the questions\\ntokenized_questions = [preprocess(question) for question in questions]\\n\\n# Train Word2Vec model\\nw2v_model = Word2Vec(sentences=tokenized_questions, vector_size=300, window=5, min_count=1, workers=4)\\n\\n# Function to convert a question to a vector by averaging word embeddings\\ndef question_to_vec(question, model):\\n    tokens = preprocess(question)\\n    vectors = [model.wv[word] for word in tokens if word in model.wv]\\n    if len(vectors) == 0:\\n        return np.zeros(model.vector_size)\\n    return np.mean(vectors, axis=0)\\n\\n# Convert each question to a vector\\nq1_vecs = np.array([question_to_vec(q, w2v_model) for q in ques_df['question1']])\\nq2_vecs = np.array([question_to_vec(q, w2v_model) for q in ques_df['question2']])\\n\""
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "\n",
    "# Function to preprocess and tokenize text\n",
    "def preprocess(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Assuming ques_df is your DataFrame containing 'question1' and 'question2' columns\n",
    "questions = list(ques_df['question1']) + list(ques_df['question2'])\n",
    "\n",
    "# Preprocess and tokenize the questions\n",
    "tokenized_questions = [preprocess(question) for question in questions]\n",
    "\n",
    "# Train Word2Vec model\n",
    "w2v_model = Word2Vec(sentences=tokenized_questions, vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Function to convert a question to a vector by averaging word embeddings\n",
    "def question_to_vec(question, model):\n",
    "    tokens = preprocess(question)\n",
    "    vectors = [model.wv[word] for word in tokens if word in model.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(model.vector_size)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Convert each question to a vector\n",
    "q1_vecs = np.array([question_to_vec(q, w2v_model) for q in ques_df['question1']])\n",
    "q2_vecs = np.array([question_to_vec(q, w2v_model) for q in ques_df['question2']])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a9ec9390-5f2c-4d39-857b-c255841b11bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 600)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_df1 = pd.DataFrame(q1_vecs, index= ques_df.index)\n",
    "temp_df2 = pd.DataFrame(q2_vecs, index= ques_df.index)\n",
    "temp_df = pd.concat([temp_df1, temp_df2], axis=1)\n",
    "temp_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5d58cffd-c8c3-43d7-8385-4a7f583f2717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 1223)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>q1_len</th>\n",
       "      <th>q2_len</th>\n",
       "      <th>q1_words</th>\n",
       "      <th>q2 _words</th>\n",
       "      <th>common_words</th>\n",
       "      <th>ttl_words</th>\n",
       "      <th>word_share</th>\n",
       "      <th>cwc_min</th>\n",
       "      <th>cwc_max</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>398782</th>\n",
       "      <td>1</td>\n",
       "      <td>75</td>\n",
       "      <td>76</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>26</td>\n",
       "      <td>0.462</td>\n",
       "      <td>0.874989</td>\n",
       "      <td>0.874989</td>\n",
       "      <td>...</td>\n",
       "      <td>0.933863</td>\n",
       "      <td>1.200200</td>\n",
       "      <td>1.851302</td>\n",
       "      <td>0.337442</td>\n",
       "      <td>1.561577</td>\n",
       "      <td>2.452665</td>\n",
       "      <td>0.724530</td>\n",
       "      <td>-0.800587</td>\n",
       "      <td>1.311738</td>\n",
       "      <td>-0.584667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115086</th>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "      <td>58</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>8</td>\n",
       "      <td>24</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.666644</td>\n",
       "      <td>0.399992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.643054</td>\n",
       "      <td>1.607599</td>\n",
       "      <td>1.981272</td>\n",
       "      <td>0.378257</td>\n",
       "      <td>0.798612</td>\n",
       "      <td>3.045165</td>\n",
       "      <td>0.770287</td>\n",
       "      <td>-0.810967</td>\n",
       "      <td>1.602576</td>\n",
       "      <td>-0.719753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327711</th>\n",
       "      <td>0</td>\n",
       "      <td>106</td>\n",
       "      <td>123</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>38</td>\n",
       "      <td>0.105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.830573</td>\n",
       "      <td>0.883170</td>\n",
       "      <td>1.807486</td>\n",
       "      <td>0.060252</td>\n",
       "      <td>1.780334</td>\n",
       "      <td>2.242856</td>\n",
       "      <td>1.272725</td>\n",
       "      <td>-1.001031</td>\n",
       "      <td>1.421049</td>\n",
       "      <td>-0.576508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367788</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>145</td>\n",
       "      <td>14</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.347821</td>\n",
       "      <td>1.171073</td>\n",
       "      <td>1.378573</td>\n",
       "      <td>0.059545</td>\n",
       "      <td>0.562599</td>\n",
       "      <td>2.284448</td>\n",
       "      <td>0.588130</td>\n",
       "      <td>-0.591262</td>\n",
       "      <td>0.792259</td>\n",
       "      <td>-0.453733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151235</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>50</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0.231</td>\n",
       "      <td>0.749981</td>\n",
       "      <td>0.599988</td>\n",
       "      <td>...</td>\n",
       "      <td>0.532987</td>\n",
       "      <td>1.088109</td>\n",
       "      <td>1.897766</td>\n",
       "      <td>-0.252572</td>\n",
       "      <td>1.052277</td>\n",
       "      <td>2.425925</td>\n",
       "      <td>0.773744</td>\n",
       "      <td>-0.688584</td>\n",
       "      <td>0.722872</td>\n",
       "      <td>-0.731374</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1223 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        is_duplicate  q1_len  q2_len  q1_words  q2 _words  common_words  \\\n",
       "398782             1      75      76        13         13            12   \n",
       "115086             0      50      58        13         16             8   \n",
       "327711             0     106     123        28         21             4   \n",
       "367788             0      58     145        14         32             1   \n",
       "151235             0      34      50         5          9             3   \n",
       "\n",
       "        ttl_words  word_share   cwc_min   cwc_max  ...       290       291  \\\n",
       "398782         26       0.462  0.874989  0.874989  ...  0.933863  1.200200   \n",
       "115086         24       0.333  0.666644  0.399992  ...  0.643054  1.607599   \n",
       "327711         38       0.105  0.000000  0.000000  ...  0.830573  0.883170   \n",
       "367788         34       0.029  0.000000  0.000000  ...  0.347821  1.171073   \n",
       "151235         13       0.231  0.749981  0.599988  ...  0.532987  1.088109   \n",
       "\n",
       "             292       293       294       295       296       297       298  \\\n",
       "398782  1.851302  0.337442  1.561577  2.452665  0.724530 -0.800587  1.311738   \n",
       "115086  1.981272  0.378257  0.798612  3.045165  0.770287 -0.810967  1.602576   \n",
       "327711  1.807486  0.060252  1.780334  2.242856  1.272725 -1.001031  1.421049   \n",
       "367788  1.378573  0.059545  0.562599  2.284448  0.588130 -0.591262  0.792259   \n",
       "151235  1.897766 -0.252572  1.052277  2.425925  0.773744 -0.688584  0.722872   \n",
       "\n",
       "             299  \n",
       "398782 -0.584667  \n",
       "115086 -0.719753  \n",
       "327711 -0.576508  \n",
       "367788 -0.453733  \n",
       "151235 -0.731374  \n",
       "\n",
       "[5 rows x 1223 columns]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.concat([final_df, temp_df], axis=1)\n",
    "print(final_df.shape)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0d1ad170-b51c-4ae2-88a4-6521aa1d40f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(final_df.iloc[:,1:].values,final_df.iloc[:,0].values,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "62ce02b6-f200-47fa-ad32-04935bee5678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7745"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(X_train,y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "17105d06-4f31-45bd-afa3-5a2ebf279af0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7688333333333334"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train,y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff00def6-4eee-4111-b889-ff31abfe8712",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
